{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from open_ai_utils import simular_respuesta_generativa\n",
    "from LLMstoDataBase import SQLAgent\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "# Ignorar todos los warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cargar variables de entorno desde .env\n",
    "load_dotenv()\n",
    "\n",
    "# Acceder a la API key\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "¿Que precio tienen las peliculas en las que ha trabajado el actor o actores con más películas, \\\n",
    "devuelve el nombre de las películas y el actor o actores que hayan trabajado en ellas?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalizar = False\n",
    "primera_consulta = True\n",
    "\n",
    "pagila_bbdd = {\n",
    "    'bbdd_name':'pagila', \n",
    "    'user'     :'postgres', \n",
    "    'password' :'123', \n",
    "    'host'     :'localhost', \n",
    "    'port'     :'5432'\n",
    "}\n",
    "\n",
    "# inicializamos el agente\n",
    "agent = SQLAgent(**pagila_bbdd)\n",
    "\n",
    "# entramos en la conversación-chat\n",
    "while finalizar == False:\n",
    "\n",
    "    conversacion = True\n",
    "    nueva_consulta = True\n",
    "\n",
    "    ### podria hacer una funcion para reconocer a que base de datos se quiere conectar el usuario\n",
    "\n",
    "    # el usuario hace la consulta\n",
    "    simular_respuesta_generativa('AGENTE:\\nSoy tu agente experto en bases de datos. ¿En que te puedo ayudar hoy?\\n\\n')\n",
    "    consulta_usuario = str(input())\n",
    "\n",
    "    while conversacion: \n",
    "        \n",
    "        if nueva_consulta== True:\n",
    "            # printeamos la consulta del usuario\n",
    "            simular_respuesta_generativa(f'\\nPROMPT_USUARIO_{agent.historico.contador_interacciones+1}:\\n\\n{consulta_usuario}\\n\\n')\n",
    "\n",
    "            # le pedimos la consulta al agente y printeamos la tabla\n",
    "            tabla_resultado, codigo_sql = agent.nlp_to_sql(\n",
    "                    consulta_nlp=consulta_usuario, \n",
    "                    metadata_token_limit= 1000, \n",
    "                            )      \n",
    "\n",
    "            simular_respuesta_generativa(f'\\nAGENTE:\\nAqui tienes el resultado de tu consulta:\\n{tabla_resultado}\\n\\n')\n",
    "\n",
    "            # generamos el informe de la consulta dandole al agente la tabla el codigo sql y la consulta del usuario\n",
    "            simular_respuesta_generativa('\\nAGENTE:\\nMe dispongo a generar el informe de tu consulta...\\n\\n')\n",
    "            informe = agent.informe_resultado(\n",
    "                    consulta_usuario= consulta_usuario, \n",
    "                    tabla_texto=tabla_resultado, \n",
    "                    max_tokens_respuesta=1000, \n",
    "                    codigo_sql= codigo_sql )\n",
    "            nueva_consulta= False\n",
    "                \n",
    "            # printeamos el informe\n",
    "            simular_respuesta_generativa(f'\\nAGENTE:\\nAqui tienes el informe de tu consulta: \\n{informe}\\n\\n')\n",
    "\n",
    "        # continuamos chat sobre la lectura o nueva consulta\n",
    "        simular_respuesta_generativa('\\nAGENTE:\\n¿Tienes alguna otra consulta?\\n\\n')\n",
    "        consulta_usuario = str(input())\n",
    "        conversacion, nueva_consulta = agent.continuar_conversando(\n",
    "            usuario= consulta_usuario, \n",
    "            tabla_consulta_anterior=tabla_resultado,\n",
    "            codigo_sql_ejecutado=codigo_sql\n",
    "            )\n",
    "\n",
    "        while conversacion == True and nueva_consulta== False:\n",
    "\n",
    "            # printeamos la consulta del usuario sobre respuestas anteriores\n",
    "            simular_respuesta_generativa(f'\\nPROMPT_USUARIO_{agent.historico.contador_interacciones +1}:\\n{consulta_usuario}\\n\\n')\n",
    "\n",
    "            respuesta_agente = agent.pregunta_sobre_consulta_anterior(\n",
    "                usuario= consulta_usuario, \n",
    "                tabla_consulta_anterior=tabla_resultado, \n",
    "                consulta_sql_anterior=codigo_sql\n",
    "                )\n",
    "            # printeamos las nuevas respuesta del sistema\n",
    "            simular_respuesta_generativa(f'\\nAGENTE:\\nLa respuesta a tu consulta:\\n{respuesta_agente}\\n\\n')\n",
    "\n",
    "            # continuamos chat sobre la lectura o nueva consulta\n",
    "            simular_respuesta_generativa('\\nAGENTE:\\n¿Tienes alguna otra consulta?\\n\\n')\n",
    "            consulta_usuario = str(input())\n",
    "            conversacion, nueva_consulta = agent.continuar_conversando(\n",
    "                usuario= consulta_usuario, \n",
    "                codigo_sql_ejecutado=codigo_sql, \n",
    "                tabla_consulta_anterior=tabla_resultado, \n",
    "                max_tokens_historico= 1500\n",
    "                )\n",
    "\n",
    "    simular_respuesta_generativa('AGENTE:\\nEstas seguro de que quieres cerrar el chat? (Y/N)\\n\\n')\n",
    "    ultima_oportunidad = str(input())\n",
    "\n",
    "    if ultima_oportunidad.lower() == 'y': \n",
    "        finalizar = True\n",
    "        agent.close_connection()\n",
    "\n",
    "simular_respuesta_generativa(f'\\n\\nHa sido un placer ayudarte. Hasta la próxima!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "ruta_chats = 'chats/'\n",
    "    # guardamos la instancia del agente\n",
    "# ruta_agente = os.path.join(ruta_chats, 'agenteSQL.pickle')\n",
    "# with open(ruta_agente, 'wb') as guardar_agente:\n",
    "#     pickle.dump(agent, guardar_agente)\n",
    "\n",
    "    # guardamos el historico en un txt \n",
    "historico_completo = agent.historico_completo.historico\n",
    "ruta_historico_chat = os.path.join(ruta_chats, 'historico.txt')\n",
    "with open(ruta_historico_chat, 'w', encoding='utf-8') as archivo_historico:\n",
    "    archivo_historico.write(historico_completo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.historico.historico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.historico_completo.historico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in agent.historico_completo.info_consultas_estructurada.items():\n",
    "    print(f'consulta_{key}')\n",
    "    for n in value:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
