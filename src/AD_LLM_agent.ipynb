{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import requests\n",
    "from flujo_de_dialogo import ventana_de_historico, guardar_historico\n",
    "from calcular_tokens import num_tokens_from_messages, num_tokens_from_string\n",
    "from parse import parse_class_predictions, parse_proba_predictions\n",
    "from open_ai_utils import simular_respuesta_generativa\n",
    "from load_store_utils import cargar_modelo \n",
    "from anomaly_detection import predict_anomalies, cargar_modelos_anomaly_detection\n",
    "from anomaly_detection import nueva_consulta_anomaly_detection\n",
    "from flujo_de_dialogo import pregunta_sobre_consulta_anterior_AD, continuar_conversacion_AD\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from load_store_utils import cargar_lote_datos\n",
    "from open_ai_utils import enviar_promt_chat_completions_mode\n",
    "import warnings\n",
    "\n",
    "# Ignorar todos los warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Cargar variables de entorno desde .env\n",
    "load_dotenv()\n",
    "\n",
    "# Acceder a la API key\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"cargar_lote_datos\",\n",
    "        \"description\": \"Carga un lote de datos de un archivo CSV basado en el número de muestras especificadas cada muestra representa 1 hora.\\\n",
    "                        Si no te especifican un numero exacto: m_samples = 5\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"m_samples\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Número de muestras a cargar\"\n",
    "                },\n",
    "                \"formato\" : {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"DataFrame\", \"numpy\"],\n",
    "                    \"description\": \"Formato en el que se devuelven los datos, DataFrame por defecto\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"m_samples\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENTE:\n",
      "Hola! en que te puedo ayudar hoy?\n",
      "\n",
      "PROMPT_USUARIO_1:\n",
      "dime cual es el estado de sistema para el último cuarto de día\n",
      "\n",
      "AGENTE:\n",
      "El número de instancias a cargar es de 6\n",
      "AGENTE:\n",
      "Los datos han sido cargados con exito! Aqui tienes la lectura de datos de tu consulta:\n",
      "\n",
      "| Timestamp           |   T_Supply |   T_Return |   SP_Return |   T_Saturation |   T_Outdoor |   RH_Supply |   RH_Return |   RH_Outdoor |   Energy |   Power |\n",
      "|:--------------------|-----------:|-----------:|------------:|---------------:|------------:|------------:|------------:|-------------:|---------:|--------:|\n",
      "| 2021-04-14 21:00:00 |     18.72  |    21.9475 |        21.5 |          17.74 |        15.2 |      38.875 |      24.86  |        53    |        7 |   2.502 |\n",
      "| 2021-04-14 21:30:00 |     19.27  |    20.68   |        21.5 |          18.9  |        14.7 |      39.38  |      26.66  |        54    |        0 |   0     |\n",
      "| 2021-04-14 22:00:00 |     19.48  |    20.22   |        20.5 |          19.35 |        14.7 |      39.095 |      27.65  |        56.25 |        0 |   0     |\n",
      "| 2021-04-14 22:30:00 |     19.54  |    20.0325 |        20.5 |          19.57 |        14.7 |      39.02  |      27.85  |        57    |        0 |   0     |\n",
      "| 2021-04-14 23:00:00 |     19.475 |    19.9525 |        20.5 |          19.48 |        13.7 |      39.21  |      28.01  |        57    |        0 |   0     |\n",
      "| 2021-04-14 23:30:00 |     19.42  |    19.91   |        20.5 |          19.4  |        13.7 |      39.6   |      28.095 |        57    |        0 |   0     |\n",
      "\n",
      "AGENTE:\n",
      "Voy a cargar los modelos!...\n",
      "Modelo cargado con éxito desde './models/anomaly_detection/scaler_trained.pickle'\n",
      "Modelo cargado con éxito desde './models/anomaly_detection/IF_trained.pickle'\n",
      "Modelo cargado con éxito desde './models/anomaly_detection/EE_trained.pickle'\n",
      "Modelo cargado con éxito desde './models/anomaly_detection/LOF_trained.pickle'\n",
      "Modelo cargado con éxito desde './models/anomaly_detection/OCSVM_trained.pickle'\n",
      "Modelo cargado con éxito desde './models/anomaly_detection/GM_trained.pickle'\n",
      "Modelo cargado con éxito desde './models/anomaly_detection/dbscan_knn.pickle'\n",
      "\n",
      "Los modelos han sido cargados con exito!\n",
      "\n",
      "AGENTE:\n",
      "Voy a escalar los datos para hacer inferencia sobre las lecturas...\n",
      "\n",
      "Los datos se han escaldo correctamente!\n",
      "\n",
      "AGENTE:\n",
      "Aqui tienes las predicciones para las lecturas: 0 lectura normal, 1 lectura inusual\n",
      "\n",
      "| Timestamp           |   anomaly |\n",
      "|:--------------------|----------:|\n",
      "| 2021-04-14 21:00:00 |         0 |\n",
      "| 2021-04-14 21:30:00 |         0 |\n",
      "| 2021-04-14 22:00:00 |         0 |\n",
      "| 2021-04-14 22:30:00 |         0 |\n",
      "| 2021-04-14 23:00:00 |         0 |\n",
      "| 2021-04-14 23:30:00 |         0 |\n",
      "\n",
      "Voy a procesar tu consulta y generar un informe...\n",
      "\n",
      "AGENTE:\n",
      "Aqui tienes tu informe:\n",
      "\n",
      "INFORME:\n",
      "- Número de registros: 6\n",
      "- Energía: Máximo 7 kWh, Mínimo 0 kWh, Media 1.16667 kWh\n",
      "- Potencia: Máximo 2.502 kW, Mínimo 0 kW, Media 0.417 kW\n",
      "- Temperatura exterior: Máximo 15.2 °C, Mínimo 13.7 °C, Media 14.45 °C\n",
      "- Temperatura de suministro: Máximo 19.54 °C, Mínimo 18.72 °C, Media 19.3175 °C\n",
      "- Humedad relativa exterior: Máximo 57 %, Mínimo 53 %, Media 55.7083 %\n",
      "\n",
      "PREDICCIÓN:\n",
      "- Todas las instancias se consideran normales (0 anomalías).\n",
      "\n",
      "CONCLUSIÓN:\n",
      "El sistema muestra lecturas estables y dentro de los rangos normales para el último cuarto del día, sin anomalías detectadas en las predicciones.\n",
      "\n",
      "AGENTE:\n",
      " ¿Tienes alguna otra consulta?\n",
      "\n",
      "Adios! ha sido un placer"
     ]
    }
   ],
   "source": [
    "conversacion = True\n",
    "modelos_cargados = False\n",
    "contador_interacciones = 1\n",
    "nueva_consulta = True\n",
    "historico_completo = ''\n",
    "\n",
    "simular_respuesta_generativa('AGENTE:\\nHola! en que te puedo ayudar hoy?\\n\\n')\n",
    "while conversacion: \n",
    "    \n",
    "    if nueva_consulta==True: \n",
    "        consulta_usuario = str(input())\n",
    "        simular_respuesta_generativa(f'PROMPT_USUARIO_{contador_interacciones}:\\n{consulta_usuario}\\n\\n')\n",
    "\n",
    "\n",
    "        extraccion_argumentos = [\n",
    "        {'role': 'system', 'content': f'Tu objetivo es extraer los argumentos necesarios para ejecutar la función que te he pasado en tools'}, \n",
    "        {'role': 'user', 'content': f'{consulta_usuario}'} ]\n",
    "\n",
    "        argumentos_extraidos_del_llm = enviar_promt_chat_completions_mode(\n",
    "                mensaje=extraccion_argumentos, \n",
    "                funciones= tools, \n",
    "                forzar_funciones= {\"type\": \"function\", \"function\": {\"name\": \"cargar_lote_datos\"}}, \n",
    "                aleatoriedad= 0, \n",
    "                probabilidad_acumulada=1)\n",
    "\n",
    "        simular_respuesta_generativa(f'AGENTE:\\nEl número de instancias a cargar es de {argumentos_extraidos_del_llm[\"m_samples\"]}\\n')\n",
    "\n",
    "        # cargando datos\n",
    "        load_data_params = {'name': 'HVAC_test_processeded.csv' , 'ruta_datos':'../data/Anomaly_detection/processed_data/'}\n",
    "        try: \n",
    "            data = cargar_lote_datos(**argumentos_extraidos_del_llm, **load_data_params)\n",
    "        except TypeError as e: \n",
    "            print(e)\n",
    "        #display(data)\n",
    "\n",
    "        columnas_base = ['T_Supply', 'T_Return', 'SP_Return', 'T_Saturation', 'T_Outdoor',\n",
    "                         'RH_Supply', 'RH_Return', 'RH_Outdoor', 'Energy', 'Power']\n",
    "\n",
    "        datos_base = data[columnas_base]\n",
    "\n",
    "        data_base_text = datos_base.to_markdown()\n",
    "        simular_respuesta_generativa(\n",
    "            f'AGENTE:\\nLos datos han sido cargados con exito! Aqui tienes la lectura de datos de tu consulta:\\n\\n{data_base_text}\\n\\n')\n",
    "\n",
    "        # guardamos también la descripcion estadistica del los datos\n",
    "        data_descr = datos_base.describe()\n",
    "        data_descr_text = data_descr.to_markdown()\n",
    "        # simular_respuesta_generativa(data_descr_text)\n",
    "        # print('\\n\\n')\n",
    "\n",
    "        # Cargando modelos\n",
    "        if modelos_cargados== False: \n",
    "\n",
    "            simular_respuesta_generativa('AGENTE:\\nVoy a cargar los modelos!...\\n')\n",
    "            scaler, models = cargar_modelos_anomaly_detection(ruta= './models/anomaly_detection/')\n",
    "            simular_respuesta_generativa('\\nLos modelos han sido cargados con exito!\\n\\n')\n",
    "            modelos_cargados = True\n",
    "\n",
    "        # escalado de datos\n",
    "        simular_respuesta_generativa('AGENTE:\\nVoy a escalar los datos para hacer inferencia sobre las lecturas...\\n\\n')\n",
    "        X_test_scl = scaler.transform(data)\n",
    "        X_test_scl_df= pd.DataFrame(X_test_scl, columns= scaler.feature_names_in_, index= data.index)\n",
    "        simular_respuesta_generativa('Los datos se han escaldo correctamente!\\n\\n')\n",
    "        \n",
    "        # predicciones del modelo\n",
    "        anomaly_predictions_df = predict_anomalies(models= models, scaled_data=X_test_scl_df)\n",
    "        anomaly_predictions_text_df = anomaly_predictions_df['anomaly'].to_markdown()\n",
    "        \n",
    "        simular_respuesta_generativa('AGENTE:\\nAqui tienes las predicciones para las lecturas: 0 lectura normal, 1 lectura inusual\\n\\n')\n",
    "        simular_respuesta_generativa(anomaly_predictions_text_df)\n",
    "\n",
    "        simular_respuesta_generativa('\\n\\nVoy a procesar tu consulta y generar un informe...\\n\\n')\n",
    "        # enviamos primera consulta al agente para que genere el informe\n",
    "        max_tokens_respuesta = 1000\n",
    "        respuesta = nueva_consulta_anomaly_detection(\n",
    "            consulta_usuario=consulta_usuario,\n",
    "            datos_base=data_base_text, \n",
    "            descr_datos=data_descr_text, \n",
    "            predicciones=anomaly_predictions_text_df, \n",
    "            max_tokens_respuesta=max_tokens_respuesta)\n",
    "        \n",
    "        \n",
    "        simular_respuesta_generativa('AGENTE:\\nAqui tienes tu informe:\\n\\n')\n",
    "        simular_respuesta_generativa(respuesta)\n",
    "\n",
    "    # actualizamos el historico y el contador de interacciones\n",
    "    historico_completo, contador_interacciones = guardar_historico(\n",
    "            historico=                 historico_completo, \n",
    "            mensaje_usuario=           consulta_usuario,\n",
    "            respuesta_sistema=         respuesta, \n",
    "            contador_interacciones=    contador_interacciones )\n",
    "    \n",
    "    # cogemos nuestra ventana de historico\n",
    "    historico_parcial = ventana_de_historico(historico_completo=historico_completo)\n",
    "    \n",
    "    # continuamos chat sobre la lectura o nueva consulta\n",
    "    simular_respuesta_generativa('\\n\\nAGENTE:\\n ¿Tienes alguna otra consulta?\\n\\n')\n",
    "    seguir_conversando = str(input())\n",
    "    continuar_conversacion, nueva_consulta = continuar_conversacion_AD(seguir_conversando)        \n",
    "\n",
    "    # si el usuario tiene preguntas sobre la vieja consulta\n",
    "    while continuar_conversacion == True and nueva_consulta== False:\n",
    "        respuesta = pregunta_sobre_consulta_anterior_AD(\n",
    "            consulta_usuario=seguir_conversando, \n",
    "            historico= historico_parcial, \n",
    "            datos_base=datos_base, \n",
    "            datos_descr=data_descr, \n",
    "            predicciones=anomaly_predictions_text_df\n",
    "            ) \n",
    "        simular_respuesta_generativa(respuesta)\n",
    "        \n",
    "        # actualizamos el historico y el contador de interacciones\n",
    "        historico_completo, contador_interacciones = guardar_historico(\n",
    "            historico=                 historico_completo, \n",
    "            mensaje_usuario=           consulta_usuario,\n",
    "            respuesta_sistema=         respuesta, \n",
    "            contador_interacciones=    contador_interacciones )\n",
    "    \n",
    "        # cogemos nuestra ventana de historico\n",
    "        historico_parcial = ventana_de_historico(historico_completo=historico_completo)\n",
    "    \n",
    "        # continuamos chat sobre la lectura o nueva consulta\n",
    "        simular_respuesta_generativa('\\n\\nAGENTE:\\n ¿Tienes alguna otra consulta?\\n\\n')\n",
    "        seguir_conversando = str(input())\n",
    "        continuar_conversacion, nueva_consulta = continuar_conversacion_AD(seguir_conversando)\n",
    "\n",
    "    historico_completo, contador_interacciones = guardar_historico(\n",
    "            historico=                 historico_completo, \n",
    "            mensaje_usuario=           seguir_conversando,\n",
    "            respuesta_sistema=         'Fin de la conversación', \n",
    "            contador_interacciones=    contador_interacciones )\n",
    "    \n",
    "    # si el usario no tiene preguntas o nuevas consultas se termina la conversacion\n",
    "    if continuar_conversacion == False:  conversacion= False \n",
    "\n",
    " \n",
    "    # conversacion= False\n",
    "simular_respuesta_generativa('Adios! ha sido un placer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'historico_completo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mhistorico_completo\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'historico_completo' is not defined"
     ]
    }
   ],
   "source": [
    "print(historico_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pregunta_sobre_consulta_anterior_AD(consulta_usuario:str, historico:str, datos_base:str, \n",
    "                                        datos_descr:str, predicciones:str, max_tokens_respuesta:int): \n",
    "    \n",
    "    '''Recibe una pregunta del usuario sobre una respuesta anterior del sistema y el agente devuelve una respuesta teniendo \\\n",
    "       acceso al historico y a la los datos de lectura, descripción y predicciones'''\n",
    "\n",
    "    prompt_conversacion = [\n",
    "    {'role': 'system', 'content': f'Eres un asistente de ayuda para un diagnostico de fallos en un sistema energético HVAC que responde de manera concisa sobre un \\\n",
    "                                    informe generado previamente. Siempre pones las unidades de las variables. El historico de la conversación: {historico}\\\n",
    "                                    recuerda: 1 significa valor típico y -1 valor atípico. Tu respuesta debe ser como maximo de {max_tokens_respuesta-100} tokens'}, \n",
    "                                \n",
    "    {'role': 'user', 'content': f'A partir de la siguiente lectura:\\n {datos_base}\\n, con descripción: \\n {datos_descr}\\n y la siguiente \\\n",
    "                                  predicción: \\n {predicciones}\\n responde a mi consulta: \\n{consulta_usuario}'}]\n",
    "    \n",
    "    respuesta = enviar_promt_chat_completions_mode(\n",
    "            mensaje= prompt_conversacion, \n",
    "            modelo=\"gpt-4-1106-preview\", \n",
    "            maximo_tokens=max_tokens_respuesta, \n",
    "            aleatoriedad=0.2, \n",
    "            probabilidad_acumulada=0.8)\n",
    "\n",
    "    return respuesta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def cargar_lote_datos(m_samples, name, ruta_datos, formato= 'DataFrame'): \n",
    "#     '''Carga un lote de m ejemplos de un archivo csv determinado de manera aleatoria, \n",
    "#        en formato dataframe o en numpy array'''\n",
    "\n",
    "#     ruta_completa = os.path.join(ruta_datos, name)\n",
    "\n",
    "#     lote = pd.read_csv(ruta_completa).set_index('Timestamp').iloc[-m_samples:, ::]\n",
    "\n",
    "#     if formato=='DataFrame': return lote\n",
    "#     else:                    return lote.to_numpy()\n",
    "\n",
    "# # cargando datos\n",
    "# load_data_params = {'name': 'HVAC_test_processeded.csv' , 'ruta_datos':'../data/Anomaly_detection/processed_data/'}\n",
    "# try: \n",
    "#     data = cargar_lote_datos(10, **load_data_params)\n",
    "# except TypeError as e: \n",
    "#    print(e)\n",
    "\n",
    "# display(data)\n",
    "\n",
    "# columnas_base = ['T_Supply', 'T_Return', 'SP_Return', 'T_Saturation', 'T_Outdoor',\n",
    "#                         'RH_Supply', 'RH_Return', 'RH_Outdoor', 'Energy', 'Power']\n",
    "\n",
    "# datos_base = data[columnas_base]\n",
    "\n",
    "# display(datos_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def continuar_conversacion_AD(respuesta_usuario):\n",
    "\n",
    "#     tools = [\n",
    "#     {\n",
    "#     \"type\": \"function\",\n",
    "#     \"function\": {\n",
    "#         \"name\": \"continuar_conversacion\",\n",
    "#         \"description\": \"Debes extraer DOS PARAMETROS clasificando una respuesta del usuario; por un lado:\\n\\\n",
    "#                         tienes que clasificar el parametro 'continuar':\\n  \\\n",
    "#                         - True (seguir chateando) si el usuario hace una pregunta o tiene alguna petición.\\n \\\n",
    "#                         - False(no seguir chateando) si el usuario no tiene ninguna pregunta o petición.\\n \\\n",
    "#                         por otro lado, si 'continuar'es True; debes clasificar el parametro 'nueva_cosulta':\\n  \\\n",
    "#                         - True (es una nueva consulta) el usuario pide una lectura sobre datos de otro momento o una nueva consulta.\\n \\\n",
    "#                         - False(es una cuestion sobre una lectura anterior) el usuario pide explicaciones sobre una consulta anterior.\\n \\\n",
    "#                         Ambos parametros son booleanos. Si 'continuar' es False, entonces 'nueva_consulta' también es False\",\n",
    "#         \"parameters\": {\n",
    "#             \"type\": \"object\",\n",
    "#             \"properties\": {\n",
    "#                 \"continuar\": {\n",
    "#                     \"type\": \"boolean\",\n",
    "#                     \"description\": \"solo puede ser True o False\"\n",
    "#                             },\n",
    "#                 'nueva_consulta':{\n",
    "#                     \"type\": \"boolean\",\n",
    "#                     \"description\": \"solo puede ser True o False\"\n",
    "#                             },\n",
    "#             },\n",
    "#             \"required\": [\"continuar\", \"nueva_consulta\"]\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# ]\n",
    "\n",
    "#     extraccion_argumentos = [\n",
    "#         {'role': 'system', 'content': f'Tu objetivo es extraer los argumentos necesarios para ejecutar la función que te he pasado en tools'}, \n",
    "#         {'role': 'user', 'content': f'{respuesta_usuario}'} ]\n",
    "\n",
    "#     argumentos_extraidos_del_llm = enviar_promt_chat_completions_mode(\n",
    "#             mensaje=extraccion_argumentos, \n",
    "#             funciones= tools, \n",
    "#             forzar_funciones= {\"type\": \"function\", \"function\": {\"name\": \"continuar_conversacion\"}}, \n",
    "#             aleatoriedad= 0, \n",
    "#             probabilidad_acumulada=1)\n",
    "    \n",
    "#     return argumentos_extraidos_del_llm['continuar'], argumentos_extraidos_del_llm['nueva_consulta']\n",
    "\n",
    "\n",
    "# respuesta_usuario_si_continuar= 'puedes explicarme por que esas lecturas se consideran anomalias?'\n",
    "# respuesta_usuario_no_continuar = 'vale muchas gracias me tengo que ir'\n",
    "# respuesta_nueva_consulta = 'quiero saber las lecturas del dia anterior'\n",
    "# respuesta_nueva_consulta_ = 'cual es el estado del sistema energético hvac para el último cuarto de dia'\n",
    "# res1, new1 = continuar_conversacion(respuesta_usuario=respuesta_usuario_si_continuar)\n",
    "# res2, new2 = continuar_conversacion(respuesta_usuario=respuesta_usuario_no_continuar)\n",
    "# res3, new3 = continuar_conversacion(respuesta_usuario=respuesta_nueva_consulta)\n",
    "# res4, new4 = continuar_conversacion(respuesta_usuario=respuesta_nueva_consulta_)\n",
    "\n",
    "# print(res1, new1)\n",
    "# print(res2, new2)\n",
    "# print(res3, new3)\n",
    "# print(res4, new4)\n",
    "#print(continuar_conversacion(str(input())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
